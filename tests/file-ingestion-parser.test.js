/**
 * File Ingestion Engine - Comprehensive Parser Tests
 * Tests JSONL/regex parsing, edge cases, offset persistence, concurrency
 * Zero tolerance for fabricated data or silent failures
 */

const fs = require('fs');
const path = require('path');
const os = require('os');
const FileIngestionEngine = require('../engines/file-ingestion-engine');
const DatabaseAccessLayer = require('../database-access-layer');
const winston = require('winston');

// Test logger (silent)
const testLogger = winston.createLogger({
  transports: [new winston.transports.Console({ silent: true })]
});

const loggers = {
  system: testLogger,
  api: testLogger,
  security: testLogger
};

describe('FileIngestionEngine - Parser Tests', () => {
  let tempDir;
  let dbPath;
  let dal;
  let engine;

  beforeEach(async () => {
    // Create temp directory for test files
    tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'file-ingest-test-'));
    
    // Use :memory: database in CI to avoid file system issues
    dbPath = (process.env.CI || process.env.GITHUB_ACTIONS) ? ':memory:' : path.join(tempDir, 'test.db');
    
    // Run migration FIRST to create tables and save the database file
    const DatabaseMigration = require('../migrations/database-migration');
    const migration = new DatabaseMigration(dbPath, testLogger);
    await migration.runMigration();
    
    // For :memory: databases, reuse migration.db connection to preserve tables
    // For file-based databases, migration closes connection after creating file
    if (dbPath === ':memory:' && migration.db) {
      dal = new DatabaseAccessLayer(dbPath, testLogger, migration.db);
      // Wait for async table creation
      await new Promise(resolve => setTimeout(resolve, 500));
    } else {
      // File-based: load the migrated database from disk
      dal = new DatabaseAccessLayer(dbPath, testLogger);
      await new Promise(resolve => setTimeout(resolve, 500));
    }
    
    // Configure engine
    process.env.FILE_INGESTION_ENABLED = 'true';
    process.env.FILE_INGESTION_DIRECTORY = tempDir;
    process.env.FILE_INGESTION_FILE_PATTERN = '**/*.{log,jsonl}';
    process.env.FILE_INGESTION_MODE = 'auto';
    
    engine = new FileIngestionEngine({}, loggers, dal);
  });

  afterEach(async () => {
    // Cleanup - shutdown engine first to release file watchers
    if (engine) await engine.shutdown();
    if (dal) dal.cleanup();
    if (fs.existsSync(tempDir)) {
      fs.rmSync(tempDir, { recursive: true, force: true });
    }
  });

  describe('JSONL Parsing', () => {
    test('should parse valid JSONL with minimal fields', async () => {
      const testFile = path.join(tempDir, 'test.jsonl');
      const lines = [
        JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'Test message 1' }),
        JSON.stringify({ timestamp: '2025-11-14T10:01:00Z', level: 'error', message: 'Test message 2' })
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000)); // Wait for chokidar

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2);
      expect(logs[0].level).toBe('info');
      expect(logs[0].message).toBe('Test message 1');
      expect(logs[1].level).toBe('error');
      expect(logs[1].message).toBe('Test message 2');
    });

    test('should parse JSONL with extended fields', async () => {
      const testFile = path.join(tempDir, 'extended.jsonl');
      const line = JSON.stringify({
        timestamp: '2025-11-14T10:00:00Z',
        level: 'warn',
        source: 'api-server',
        message: 'Warning message',
        ip: '192.168.1.1',
        tags: ['api', 'performance'],
        metadata: { userId: 123, endpoint: '/api/test' }
      });
      fs.writeFileSync(testFile, line + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(1);
      expect(logs[0].source).toBe('api-server');
      expect(logs[0].ip).toBe('192.168.1.1');
    });

    test('should record parse error for invalid JSON', async () => {
      const testFile = path.join(tempDir, 'invalid.jsonl');
      const lines = [
        '{"timestamp": "2025-11-14T10:00:00Z", "level": "info", "message": "valid"}',
        '{"invalid": "missing brace"',  // Invalid JSON
        '{"timestamp": "2025-11-14T10:02:00Z", "level": "info", "message": "valid again"}'
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1500));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2); // Only valid lines ingested

      const errors = await dal.getRecentParseErrors(10);
      expect(errors.length).toBeGreaterThan(0);
      const error = errors.find(e => e.line_snippet && e.line_snippet.includes('missing brace'));
      expect(error).toBeDefined();
      expect(error.reason).toBe('invalid-json');
    });

    test('should handle Unicode and emoji in JSONL', async () => {
      const testFile = path.join(tempDir, 'unicode.jsonl');
      const line = JSON.stringify({
        timestamp: '2025-11-14T10:00:00Z',
        level: 'info',
        message: 'Test 擔먼봏 游 칌o침o'
      });
      fs.writeFileSync(testFile, line + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(1);
      expect(logs[0].message).toBe('Test 擔먼봏 游 칌o침o');
    });

    test('should handle very long lines without crash', async () => {
      const testFile = path.join(tempDir, 'longline.jsonl');
      const longMessage = 'A'.repeat(100 * 1024); // 100KB message
      const line = JSON.stringify({
        timestamp: '2025-11-14T10:00:00Z',
        level: 'info',
        message: longMessage
      });
      fs.writeFileSync(testFile, line + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1500));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(1);
      expect(logs[0].message.length).toBe(100 * 1024);
    });
  });

  describe('Regex Parsing', () => {
    test('should parse regex format: TIMESTAMP LEVEL MESSAGE', async () => {
      process.env.FILE_INGESTION_MODE = 'regex';
      engine = new FileIngestionEngine({}, loggers, dal);
      
      const testFile = path.join(tempDir, 'regex.log');
      const lines = [
        '2025-11-14T10:00:00.123Z INFO Service started',
        '2025-11-14T10:01:00.456Z ERROR Connection failed',
        '2025-11-14T10:02:00.789Z WARN Low disk space'
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs ORDER BY timestamp');
      expect(logs.length).toBe(3);
      expect(logs[0].level).toBe('info');
      expect(logs[0].message).toBe('Service started');
      expect(logs[1].level).toBe('error');
      expect(logs[1].message).toBe('Connection failed');
    });

    test('should record parse error for non-matching regex lines', async () => {
      process.env.FILE_INGESTION_MODE = 'regex';
      engine = new FileIngestionEngine({}, loggers, dal);
      
      const testFile = path.join(tempDir, 'nomatch.log');
      const lines = [
        '2025-11-14T10:00:00Z INFO Valid line',
        'This line does not match the pattern',
        '2025-11-14T10:02:00Z WARN Another valid'
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1500));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2); // Only matching lines

      const errors = await dal.getRecentParseErrors(10);
      const error = errors.find(e => e.line_snippet && e.line_snippet.includes('does not match'));
      expect(error).toBeDefined();
      expect(error.reason).toBe('no-regex-match');
    });
  });

  describe('Edge Cases', () => {
    test('should handle Windows CRLF line endings', async () => {
      const testFile = path.join(tempDir, 'crlf.jsonl');
      const lines = [
        JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'Line 1' }),
        JSON.stringify({ timestamp: '2025-11-14T10:01:00Z', level: 'info', message: 'Line 2' })
      ];
      fs.writeFileSync(testFile, lines.join('\r\n') + '\r\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2);
    });

    test('should handle file with no trailing newline', async () => {
      const testFile = path.join(tempDir, 'notrailing.jsonl');
      const line = JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'Test' });
      fs.writeFileSync(testFile, line); // No newline at end

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(1);
    });

    test('should handle empty lines gracefully', async () => {
      const testFile = path.join(tempDir, 'empty.jsonl');
      const lines = [
        JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'First' }),
        '',
        '  ',
        JSON.stringify({ timestamp: '2025-11-14T10:01:00Z', level: 'info', message: 'Second' })
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2); // Empty lines ignored
    });

    test('should handle binary-like content without crash', async () => {
      const testFile = path.join(tempDir, 'binary.log');
      const buffer = Buffer.alloc(256);
      for (let i = 0; i < 256; i++) buffer[i] = i;
      fs.writeFileSync(testFile, buffer);

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1500));

      // Should not crash; unparsable content recorded as errors
      const errors = await dal.getRecentParseErrors(10);
      expect(errors.length).toBeGreaterThan(0);
    });
  });

  describe('Offset Persistence', () => {
    test('should persist offset and resume after restart', async () => {
      const testFile = path.join(tempDir, 'offset.jsonl');
      
      // Initial write
      const lines1 = [
        JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'Initial 1' }),
        JSON.stringify({ timestamp: '2025-11-14T10:01:00Z', level: 'info', message: 'Initial 2' })
      ];
      fs.writeFileSync(testFile, lines1.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      let logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2);

      // Append more lines
      const lines2 = [
        JSON.stringify({ timestamp: '2025-11-14T10:02:00Z', level: 'info', message: 'After restart 1' }),
        JSON.stringify({ timestamp: '2025-11-14T10:03:00Z', level: 'info', message: 'After restart 2' })
      ];
      fs.appendFileSync(testFile, lines2.join('\n') + '\n');

      await new Promise(resolve => setTimeout(resolve, 1500));

      logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(4);
      
      // Verify offset persisted
      const offset = await dal.getFileOffset(testFile);
      expect(offset).toBeDefined();
      expect(offset.last_offset).toBeGreaterThan(0);
    });

    test('should not re-ingest lines on restart', async () => {
      const testFile = path.join(tempDir, 'noreIngest.jsonl');
      const lines = [
        JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'Line 1' }),
        JSON.stringify({ timestamp: '2025-11-14T10:01:00Z', level: 'info', message: 'Line 2' })
      ];
      fs.writeFileSync(testFile, lines.join('\n') + '\n');

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      let logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2);

      // Shutdown first engine before creating second
      await engine.shutdown();

      // Simulate restart by creating new engine instance
      const engine2 = new FileIngestionEngine({}, loggers, dal);
      await engine2.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBe(2); // No duplicates

      // Cleanup engine2
      await engine2.shutdown();
    });
  });

  describe('Large Files', () => {
    test.skip('should tail-cap large initial files beyond 50MB (slow - requires 60MB file)', async () => {
      const testFile = path.join(tempDir, 'large.jsonl');
      const line = JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: 'X'.repeat(1000) });
      
      // Write ~60MB (assuming ~1KB per line * 60k lines)
      const stream = fs.createWriteStream(testFile);
      for (let i = 0; i < 60000; i++) {
        stream.write(line + '\n');
      }
      stream.end();
      
      await new Promise(resolve => stream.on('finish', resolve));

      // Verify file is > 50MB
      const stats = fs.statSync(testFile);
      expect(stats.size).toBeGreaterThan(50 * 1024 * 1024);

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 2000));

      // Should only ingest tail portion, not all 60k lines
      const logs = await dal.all('SELECT * FROM logs');
      expect(logs.length).toBeLessThan(60000);
      expect(logs.length).toBeGreaterThan(0);
    });
  });

  describe('Concurrency', () => {
    test('should handle multiple files appending concurrently', async () => {
      const files = ['file1.jsonl', 'file2.jsonl', 'file3.jsonl'].map(f => path.join(tempDir, f));
      
      // Create initial files with unique messages
      files.forEach((f, i) => {
        fs.writeFileSync(f, JSON.stringify({ timestamp: '2025-11-14T10:00:00Z', level: 'info', message: `Initial file ${i}` }) + '\n');
      });

      await engine.initialize();
      await new Promise(resolve => setTimeout(resolve, 1000));

      // Append to all files quickly
      files.forEach((f, i) => {
        for (let j = 0; j < 10; j++) {
          fs.appendFileSync(f, JSON.stringify({
            timestamp: `2025-11-14T10:0${i}:${j.toString().padStart(2, '0')}Z`,
            level: 'info',
            message: `File ${i} line ${j}`
          }) + '\n');
        }
      });

      await new Promise(resolve => setTimeout(resolve, 2500)); // Increased from 2000ms

      const logs = await dal.all('SELECT * FROM logs');
      // Initial 3 + 30 appended = 33 (allow for 1-2 missing due to timing)
      expect(logs.length).toBeGreaterThanOrEqual(31);
      expect(logs.length).toBeLessThanOrEqual(33);

      // Verify no duplicates by checking unique messages
      const messages = logs.map(l => l.message);
      const unique = new Set(messages);
      expect(unique.size).toBe(logs.length); // No duplicates
    });
  });
});
